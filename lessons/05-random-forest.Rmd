# Random Forests

## Load packages

```{r load_packages}
# randomForest
library(ranger)
# rborist
library(vip) # vip - 
library(ggplot2)
```

## Load data 

Load `train_x_class`, `train_y_class`, `test_x_class`, and `test_y_class` variables we defined in 02-preprocessing.Rmd for this *classification* task. 

```{r setup_data}
# Objects: task_reg, task_class
load("data/preprocessed.RData")
```

## Overview

The random forest algorithm seeks to improve on the performance of a single decision tree by taking the average of many trees. Thus, a random forest can be viewed as an **ensemble** method, or model averaging approach. The algorithm was invented by UC Berkeley's own Leo Breiman in 2001, who was also a co-creator of decision trees (see his [1984 CART book](https://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418)).  

Random forests are an extension of **bagging**, in which multiple samples of the original data are drawn with replacement (aka "bootstrap samples"). 1996 vs 2001 RF. 1997 random subspace. An algorithm is fit separately to each sample, then the average of those estimates is used for prediction. While bagging can be used by any algorithm, random forest uses decision trees as its base learner.

Random forests add another level of randomness by also randomly sampling the features (or covariates) at each split in each decision tree. This makes the decision trees use different covariates and therefore be more unique. As a result, the average of these trees tends to be more accurate overall.

## Fit model

Fit a random forest model that predicts the number of people with heart disease using the other variables as our X predictors. If our Y variable is a factor, `ranger` will by default perform classification; if it is numeric/integer regression will be performed and if it is omitted it will run an unsupervised analysis.

```{r rf_fit}
set.seed(1)
(rf1 = ranger::ranger(train_y_class ~ ., 
                   data = train_x_class, 
                   # Number of trees
                   num.trees = 500, 
                   # Number of variables randomly sampled as candidates at each split.
                    mtry = 5,  # classification (1/0): sqrt(p); regression: p / 3
                   # Grow a probability forest?
                   probability = TRUE,
                   # We want the importance of predictors to be assessed.
                   importance = "permutation"))
```

The "OOB estimate of error rate" shows us how accurate our model is. $accuracy = 1 - error rate$. OOB stands for "out of bag" - and bag is short for "bootstrap aggregation". So OOB estimates performance by comparing the predicted outcome value to the actual value across all trees using only the observations that were not part of the training data for that tree.

We can examine the relative variable importance in table and graph form, but without all the hard coding that we did in 04-decision-trees.Rmd. Random Forest estimates variable importance by separately examining each variable and estimating how much the model's accuracy drops when that variable's values are randomly shuffled (permuted). The shuffling temporarily removes any relationship between that covariate's value and the outcome. If a variable is important then the model's accuracy will suffer a large drop when it is randomly shuffled. But if the model's accuracy doesn't change it means the variable is not important to the model - e.g. maybe it was never even chosen as a split in any of the decision trees.

## Investigate Results

```{r rf_varimp_plot}
vip::vip(rf1) + theme_bw()

# Raw data
vip::vi(rf1)

# Unhashtag to see all variables - tibbles are silly!
# View(vip::vi(rf1))
```

Read up on the [gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient) here. It's basically a measure of diversity or dispersion - a higher gini means the model is classifying better. The gini version does not randomly shuffle the variable values.  

Now, the goal is to see how the model performs on the test dataset:

```{r}
# This will predict the outcome class.
predicted_label = as.integer(predict(rf1, data = test_x_class)$predictions[, 1] > 0.5)

str(predicted_label)

table(predicted_label, test_y_class)
```

Check the accuracy of the test set:
```{r prob_hist}

mean(predicted_label == test_y_class) 

# We can also generated probability predictions, which are more granular.
predicted_prob = as.data.frame(predict(rf1, data = test_x_class)$predictions)

colnames(predicted_prob) = c("no", "yes")

summary(predicted_prob)

ggplot(predicted_prob, aes(x = yes)) + 
  geom_histogram() + 
  theme_minimal()

# TODO: add terminal node count in for ranger.
```

How did it do? Are the accuracies for the training and test sets similar?  

## Performance plateau

```{r convergence}
library(mlr)
library(OOBCurve)

set.seed(1, "L'Ecuyer-CMRG")

# mlr wants covariates and outcome to be in the same dataframe.

# For classification RF needs Y to be a factor.
# We use the best mtry based on the CV.SL results from the final prediction library.
# Takes 1 second.
(rf_time = system.time({
  # Ranger uses all available threads by default, nice.
  y = as.factor(task_class$data[[task_class$outcome]]) 
  rf = ranger::ranger(y ~ . ,
                      data = task_class$data[, task_class$covariates],
                      num.threads = RhpcBLASctl::get_num_cores(),
                      # Need this option for OOB curve analysis.
                      keep.inbag = TRUE,
                      num.trees = 4000,
                      # Could also do importance = "impurity".
                      importance = "permutation",
                      # Set based on separate grid/random search.
                      mtry = 4L,
                      # Set based on separate grid/random search.
                      min.node.size = 5L)
}))

oob_data = task_class$data[, c(task_class$outcome, task_class$covariates), drop = FALSE]

# Outcome needs to be a factor.
oob_data[[task_class$outcome]] = as.factor(task_class$data[[task_class$outcome]])


task = makeClassifTask(data = oob_data, target = task_class$outcome)
# Current package has a bug such that multiple measures have to be specified.
# We aren't using the Brier score though.
# TODO: these results could be averaged over multiple random shufflings
# of the tree ordering. Would give a more accurate, smoother curve.
# This takes ~10 seconds.
system.time({
  results = OOBCurve(rf, measures = list(mlr::auc, mlr::brier), task = task,
                     data = oob_data)
})

```

### Interpet

```{r rf_interpret}
# Look at the OOB AUC with the maximum number of trees.
# 0.89
(rf_auc = results$auc[length(results$auc)])

# Can zoom in to certain segments of the forest indexed by an ntree range.
tree_start = 3
#tree_start = 10
tree_end = length(results$auc)
x_span = seq(tree_start, tree_end)
y_span = results$auc[x_span]

ggplot(mapping = aes(x = x_span, y = y_span)) + geom_line() + theme_minimal() +
  coord_fixed(ratio = 3) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0.5, 0.94)) +
  scale_x_log10(breaks = c(3, 10, 30, 100, 300, 1000, 3000),
                limits = c(3, 4000),
                minor_breaks = NULL) +
  labs(x = "Trees in the random forest", y = "Out of Bag AUC")

```

## Challenge 3
Open Challenge 3 in the "Challenges" folder. 
